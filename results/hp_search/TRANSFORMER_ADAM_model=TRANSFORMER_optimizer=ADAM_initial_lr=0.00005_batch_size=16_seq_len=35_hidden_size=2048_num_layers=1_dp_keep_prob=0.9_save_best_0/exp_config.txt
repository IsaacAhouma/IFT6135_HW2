batch_size    16
code_file    ptb-lm.py
data    data
debug    False
dp_keep_prob    0.9
emb_size    200
evaluate    False
hidden_size    2048
initial_lr    5e-05
load_weights    None
model    TRANSFORMER
num_epochs    40
num_layers    1
optimizer    ADAM
save_best    True
save_dir    TRANSFORMER_ADAM_model=TRANSFORMER_optimizer=ADAM_initial_lr=0.00005_batch_size=16_seq_len=35_hidden_size=2048_num_layers=1_dp_keep_prob=0.9_save_best_0
seed    1111
seq_len    35
